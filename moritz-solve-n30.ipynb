{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: \\\\wsl.localhost\\Ubuntu\\home\\moritz\\maths-for-ml\n",
      "(200000, 30)\n",
      "(50000, 30)\n",
      "(200000, 30) (200000, 30)\n",
      "(50000, 30) (50000, 30)\n",
      "(200000, 60)\n",
      "(50000, 60)\n",
      "Epoch 1/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.5015 - loss: 0.7129 - val_accuracy: 0.5046 - val_loss: 0.6965\n",
      "Epoch 2/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 996us/step - accuracy: 0.5028 - loss: 0.6947 - val_accuracy: 0.4963 - val_loss: 0.6966\n",
      "Epoch 3/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.5016 - loss: 0.6940 - val_accuracy: 0.4951 - val_loss: 0.6938\n",
      "Epoch 4/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.5012 - loss: 0.6936 - val_accuracy: 0.4956 - val_loss: 0.6935\n",
      "Epoch 5/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 974us/step - accuracy: 0.5046 - loss: 0.6933 - val_accuracy: 0.4982 - val_loss: 0.6936\n",
      "Epoch 6/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 958us/step - accuracy: 0.5014 - loss: 0.6934 - val_accuracy: 0.4958 - val_loss: 0.6937\n",
      "Epoch 7/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 951us/step - accuracy: 0.5055 - loss: 0.6932 - val_accuracy: 0.4989 - val_loss: 0.6935\n",
      "Epoch 8/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 945us/step - accuracy: 0.5068 - loss: 0.6931 - val_accuracy: 0.5044 - val_loss: 0.6932\n",
      "Epoch 9/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 962us/step - accuracy: 0.5075 - loss: 0.6930 - val_accuracy: 0.5024 - val_loss: 0.6934\n",
      "Epoch 10/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 946us/step - accuracy: 0.5090 - loss: 0.6929 - val_accuracy: 0.5073 - val_loss: 0.6932\n",
      "Epoch 11/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 948us/step - accuracy: 0.5094 - loss: 0.6929 - val_accuracy: 0.4974 - val_loss: 0.6933\n",
      "Epoch 12/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 985us/step - accuracy: 0.5108 - loss: 0.6928 - val_accuracy: 0.5021 - val_loss: 0.6934\n",
      "Epoch 13/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 972us/step - accuracy: 0.5113 - loss: 0.6928 - val_accuracy: 0.4972 - val_loss: 0.6936\n",
      "Epoch 14/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 957us/step - accuracy: 0.5136 - loss: 0.6928 - val_accuracy: 0.5039 - val_loss: 0.6933\n",
      "Epoch 15/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 956us/step - accuracy: 0.5152 - loss: 0.6926 - val_accuracy: 0.5047 - val_loss: 0.6933\n",
      "Epoch 16/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 949us/step - accuracy: 0.5162 - loss: 0.6924 - val_accuracy: 0.5012 - val_loss: 0.6936\n",
      "Epoch 17/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 948us/step - accuracy: 0.5165 - loss: 0.6924 - val_accuracy: 0.5027 - val_loss: 0.6940\n",
      "Epoch 18/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 970us/step - accuracy: 0.5180 - loss: 0.6921 - val_accuracy: 0.4969 - val_loss: 0.6937\n",
      "Epoch 19/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 978us/step - accuracy: 0.5182 - loss: 0.6922 - val_accuracy: 0.5003 - val_loss: 0.6939\n",
      "Epoch 20/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 956us/step - accuracy: 0.5217 - loss: 0.6918 - val_accuracy: 0.5005 - val_loss: 0.6943\n",
      "Epoch 21/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 967us/step - accuracy: 0.5196 - loss: 0.6918 - val_accuracy: 0.4997 - val_loss: 0.6950\n",
      "Epoch 22/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.5219 - loss: 0.6918 - val_accuracy: 0.5024 - val_loss: 0.6946\n",
      "Epoch 23/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.5230 - loss: 0.6916 - val_accuracy: 0.4989 - val_loss: 0.6946\n",
      "Epoch 24/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 998us/step - accuracy: 0.5247 - loss: 0.6913 - val_accuracy: 0.5011 - val_loss: 0.6952\n",
      "Epoch 25/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.5236 - loss: 0.6915 - val_accuracy: 0.4999 - val_loss: 0.6956\n",
      "Epoch 26/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.5251 - loss: 0.6912 - val_accuracy: 0.5046 - val_loss: 0.6945\n",
      "Epoch 27/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.5272 - loss: 0.6909 - val_accuracy: 0.5027 - val_loss: 0.6952\n",
      "Epoch 28/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 992us/step - accuracy: 0.5296 - loss: 0.6906 - val_accuracy: 0.5041 - val_loss: 0.6948\n",
      "Epoch 29/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 973us/step - accuracy: 0.5299 - loss: 0.6908 - val_accuracy: 0.5006 - val_loss: 0.6950\n",
      "Epoch 30/150\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.5290 - loss: 0.6905 - val_accuracy: 0.4997 - val_loss: 0.6961\n",
      "Epoch 31/150\n",
      "\u001b[1m2892/5625\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 1000us/step - accuracy: 0.5307 - loss: 0.6906"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Load datasets and initialize models\n",
    "for n in [30]:\n",
    "    # Load dataset\n",
    "    import os\n",
    "    print(\"Current Directory:\", os.getcwd())\n",
    "    Xs = np.load(f\"/home/moritz/maths-for-ml/Kryptonite-N/Datasets/additional-kryptonite-{n}-X.npy\")\n",
    "    Ys = np.load(f\"/home/moritz/maths-for-ml/Kryptonite-N/Datasets/additional-kryptonite-{n}-y.npy\")\n",
    "    df_x = pd.DataFrame(Xs)\n",
    "    df_y = pd.Series(Ys)\n",
    "    df_x_transformed = pd.DataFrame()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train_transformed = pd.DataFrame()\n",
    "    X_test_transformed = pd.DataFrame()\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "\n",
    "    for column in df_x.columns:\n",
    "        gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "        gmm.fit(X_train[[column]])\n",
    "        proba = gmm.predict_proba(X_train[[column]])\n",
    "        X_train_transformed[f'{column}_Mode_Prob'] = np.where(proba[:, 0] > proba[:, 1], -proba[:, 0], proba[:, 1])\n",
    "        proba = gmm.predict_proba(X_test[[column]])\n",
    "        X_test_transformed[f'{column}_Mode_Prob'] = np.where(proba[:, 0] > proba[:, 1], -proba[:, 0], proba[:, 1])\n",
    "    \n",
    "    print((X_train >= 0.5).astype(int).shape, X_train_transformed.shape)\n",
    "    print((X_test >= 0.5).astype(int).shape, X_test_transformed.shape)\n",
    "    X_train = pd.concat([(X_train >= 0.5).astype(int).reset_index(drop=True), X_train_transformed.reset_index(drop=True)], axis=1)\n",
    "    X_test = pd.concat([(X_test >= 0.5).astype(int).reset_index(drop=True), X_test_transformed.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    # Split data\n",
    "    \n",
    "    # Build the neural network model\n",
    "   \n",
    "\n",
    "    model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "\n",
    "    Dense(256),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Dense(128),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Dense(64),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define a callback to stop training when accuracy reaches 95%\n",
    "    class EarlyStoppingByAccuracy(Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if logs.get('accuracy') >= 0.94:\n",
    "                print(\"\\nReached 94% accuracy, stopping training!\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=150,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1, \n",
    "        verbose=1,\n",
    "        callbacks=[EarlyStoppingByAccuracy()]\n",
    "    )\n",
    "\n",
    "    # Make predictions and evaluate\n",
    "    y_pred_nn = (model.predict(X_test) > 0.5).astype(int)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "    print(f\"Accuracy of the Neural Network Classifier on test set for n = {n}: {accuracy_nn:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_nn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as moritz-n24\n",
    "model.save('moritz-n24.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
